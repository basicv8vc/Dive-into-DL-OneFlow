
# 关键组件

首先，我们想让大家更清楚地了解一些核心组件。
无论我们遇到什么类型的机器学习问题，这些组件都将伴随我们左右：

1. 我们可以学习的*数据*（data）。
1. 如何转换数据的*模型*（model）。
1. 一个*目标函数*（objective function），用来量化模型的有效性。
1. 调整模型参数以优化目标函数的算法。

# 1.2.1 数据

毋庸置疑，如果没有数据，那么数据科学毫无用武之地。
每个数据集由一个个*样本*（example）组成，大多时候，它们遵循独立同分布(independently and identically distributed, i.i.d.)。
样本有时也叫做*数据点*（data point）或者*数据实例*（data instance），通常每个样本由一组称为*特征*（features，或*协变量*（covariates））的属性组成。
机器学习模型会根据这些属性进行预测。
在上面的监督学习问题中，要预测的是一个特殊的属性，它被称为*标签*（label，或*目标*（target））。

假设我们处理的是图像数据，每一张单独的照片即为一个样本，它的特征由每个像素数值的有序列表表示。
比如，$200\times 200$彩色照片由$200\times200\times3=120000$个数值组成，其中的“3”对应于每个空间位置的红、绿、蓝通道的强度。
再比如，对于一组医疗数据，给定一组标准的特征(如年龄、生命体征和诊断)，我们可能用此数据尝试预测患者是否会存活。

当每个样本的特征类别数量都是相同的，所以其特征向量是固定长度的，这个长度被称为数据的*维数*（dimensionality）。
固定长度的特征向量是一个方便的属性，它有助于我们量化学习大量样本。

然而，并不是所有的数据都可以用“固定长度”的向量表示。
以图像数据为例，如果它们全部来自标准显微镜设备，那么“固定长度”是可取的；
但是如果我们的图像数据来自互联网，我们不能天真的假想它们都有相同的分辨率或形状。
这时，我们可以考虑将图像裁剪成标准尺寸，但这种办法很局限，数据有丢失信息的风险。
此外，文本数据更不符合“固定长度”的要求。
考虑一下亚马逊等电子商务网站上的客户评论：有些文本数据是简短的（比如“好极了”）；有些则长篇大论。
与传统机器学习方法相比，深度学习的一个主要优势是可以处理不同长度的数据。

一般来说，我们拥有的数据越多，我们的工作就越容易。
当我们有了更多的数据，我们通常可以训练出更强大的模型，从而减少对预先设想假设的依赖。
数据集的由小变大为现代深度学习的成功奠定基础。
在没有大数据集的情况下，许多令人兴奋的深度学习模型黯然失色。
就算一些深度学习模型在小数据集上能够工作，但其效能并不比传统方法高。

请注意，仅仅拥有海量的数据是不够的，我们还需要正确的数据。
如果数据中充满了错误，或者如果数据的特征不能预测任务目标，那么模型很可能无效。
有一句古语很好地反映了这个现象：“输入的是垃圾,输出的也是垃圾。”（“Garbage in, garbage out.”）
此外，糟糕的预测性能甚至会加倍放大事态的严重性。
在一些敏感应用中，如预测性监管、简历筛选和用于贷款的风险模型，我们必须特别警惕垃圾数据的后果。
一种常见的问题来自不均衡的数据集，比如在一个有关医疗的训练数据集中，某些人群没有样本表示。
想象一下，假设你要训练一个皮肤癌识别模型，但它（在训练数据集）从未见过的黑色皮肤的人群，就会顿时束手无策。

再比如，如果用“过去的招聘决策数据”来训练一个筛选简历的模型，那么机器学习模型可能会无意中捕捉到历史残留的不公正，并将其自动化。
然而，这一切都可能在不知情的情况下发生。
因此，当数据不具有充分代表性，甚至包含了一些社会偏见时，模型就很有可能失败。


# 1.2.2 模型

大多数机器学习会涉及到数据的转换。
比如，我们建立一个“摄取照片并预测笑脸”的系统。再比如，我们摄取一组传感器读数，并预测读数的正常与异常程度。
虽然简单的模型能够解决如上简单的问题，但本书中关注的问题超出了经典方法的极限。
深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为*深度学习*（deep learning）。
在讨论深度模型的过程中，我们也将提及一些传统方法。


# 1.2.3 目标函数

前面，我们将机器学习介绍为“从经验中学习”。
这里所说的“学习”，是指自主提高模型完成某些任务的效能。
但是，什么才算真正的提高呢？
在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，我们称之为*目标函数*（objective function）。
我们通常定义一个目标函数，并希望优化它到最低点。因为越低越好，所以这些函数有时被称为*损失函数*（loss function, 或cost function）。
但这只是一个惯例，你也可以取一个新的函数，优化到它的最高点。这两个函数本质上是相同的，只是翻转一下符号。

当任务为试图预测数值时，最常见的损失函数是*平方误差*（squared error），即预测值与实际值之差的平方。
当试图解决分类问题时，最常见的目标函数是最小化错误率，即预测与实际情况不符的样本比例。
有些目标函数（如平方误差）很容易被优化，有些目标（如错误率）由于不可微性或其他复杂性难以直接优化。
在这些情况下，通常会优化*替代目标*。

通常，损失函数是根据模型参数定义的，并取决于数据集。
在一个数据集上，我们通过最小化总损失来学习模型参数的最佳值。
该数据集由一些为训练而收集的样本组成，称为*训练数据集*（training dataset，或称为*训练集*（training set））。
然而，在训练数据上表现良好的模型，并不一定在“新数据集”上有同样的效能，这里的“新数据集”通常称为*测试数据集*（test dataset，或称为*测试集*（test set））。

综上所述，我们通常将可用数据集分成两部分：训练数据集用于拟合模型参数，测试数据集用于评估拟合的模型。
然后我们观察模型在这两部分数据集的效能。
你可以把“一个模型在训练数据集上的效能”想象成“一个学生在模拟考试中的分数”。
这个分数用来为一些真正的期末考试做参考，即使成绩令人鼓舞，也不能保证期末考试成功。
换言之，测试性能可能会显著偏离训练性能。
当一个模型在训练集上表现良好，但不能推广到测试集时，我们说这个模型是“过拟合”（overfitting）的。
就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。


# 1.2.4 优化算法

一旦我们获得了一些数据源及其表示、一个模型和一个合适的损失函数，我们接下来就需要一种算法，它能够搜索出最佳参数，以最小化损失函数。
深度学习中，大多流行的优化算法通常基于一种基本方法--*梯度下降*（gradient descent）。
简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果你仅对该参数进行少量变动，训练集损失会朝哪个方向移动。
然后，它在可以减少损失的方向上优化参数。

